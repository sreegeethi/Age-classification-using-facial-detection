{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of 5_deep_learning_final_CNN_model.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ndBDuhivUu7C","executionInfo":{"status":"ok","timestamp":1602512162224,"user_tz":-330,"elapsed":37633,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"2b15dc18-fde3-4461-e515-d486b2c8fd8a","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@title Mount Google Drive {display-mode: \"form\"}\n","\n","# This code will be hidden when the notebook is loaded.\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_kTFrNNPU7wP"},"source":["# Imports\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import cv2\n","import os  # to access files in the opereating system...(here we are going to access files from the drive only)\n","from zipfile import ZipFile   # to extraxt information from zipfile\n","import time\n","from datetime import datetime\n","import itertools   #used for working with iterable data sets\n","\n","from sklearn.model_selection import train_test_split  #Quick utility that wraps input validation and next(ShuffleSplit(). split(X, y)) and application to input data into a single call for splitting (and optionally subsampling) data in a oneliner\n","from sklearn.metrics import confusion_matrix\n","\n","#to create dataflow graphs—structures that describe how data moves through a graph, or a series of processing nodes\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Conv2D, AveragePooling2D, GlobalAveragePooling2D\n","from tensorflow.keras import utils\n","from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n","\n","# Setting random seeds to reduce the amount of randomness in the neural net weights and results.\n","np.random.seed(42)\n","tf.random.set_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FXc8h48VtsX6","executionInfo":{"status":"ok","timestamp":1602512661613,"user_tz":-330,"elapsed":1925,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"12eff111-0984-4b33-cf8e-d77f7ec68138","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Checking the installed version of TensorFlow.\n","tf.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.3.0'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"uAuTEWUiOgUF"},"source":["**Ensure that the Runtime Type for this notebook is set to GPU**.\n","\n","Run the code below to check for GPU. If a GPU device is not found, change the runtime type under *Runtime* &#10141; *Change runtime type* &#10141; *Hardware accelerator* &#10141; *GPU* and run the notebook from the beginning again."]},{"cell_type":"code","metadata":{"id":"ggkatHAQU75X","executionInfo":{"status":"ok","timestamp":1602512744014,"user_tz":-330,"elapsed":7673,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"76cf1bd9-00e4-4205-c822-d77f7a856137","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Testing to ensure GPU is being utilized.\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","    raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jmgbfqAKOsxR"},"source":["## Deep Learning: Building Final CNN Model"]},{"cell_type":"markdown","metadata":{"id":"RxEzjKuOPr5n"},"source":["Below is the **summary of all the CNN models built so far** (in previous notebook ***4_deep_learning_CNN_modelling.ipynb***).\n","\n","![summary_table.png](https://drive.google.com/uc?export=view&id=1D6iYAbVaQFDo2ek4Bsj00BjxwviHZvlw)\n","\n","After understanding the effects of all the improvement and optimization techniques on the CNN model performance, **the final CNN model can now be defined and trained**:\n","1. with **grayscale images** instead of RGB coloured images.\n","2. with **augmented training dataset (234,400 images)** instead of original training dataset (23,440 images).\n","3. for **60 epochs**.\n","4. for **re-distributed classes of age-ranges**.\n","5. with an **optimized architecture**, comprising of:\n","- an input *Conv2D* layer (with 32 filters) paired with an *AveragePooling2D* layer,\n","- 3 pairs of *Conv2D* (with 64, 128 & 256 filters) and *AveragePooling2D* layers,\n","- a *GlobalAveragePooling2D* layer,\n","- 1 *Dense* layer with 132 nodes, and\n","- an output *Dense* layer with 7 nodes."]},{"cell_type":"code","metadata":{"id":"tCN57I6AU7_g","executionInfo":{"status":"ok","timestamp":1602512804453,"user_tz":-330,"elapsed":10707,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"fcb2687f-cc59-4859-b51e-8950aad13ea9","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Unzipping the dataset file combined_faces.zip\n","\n","combined_faces_zip_path = \"/content/drive/My Drive/Faces_Dataset/ZIPPED_DATASET/combined_faces.zip\"\n","\n","with ZipFile(combined_faces_zip_path, 'r') as myzip:\n","    myzip.extractall()\n","    print('Done unzipping combined_faces.zip')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Done unzipping combined_faces.zip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q61d9-wxU8Ea","executionInfo":{"status":"ok","timestamp":1602513114117,"user_tz":-330,"elapsed":280105,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"e7a00fe5-06f4-4cea-ad7d-8f2787277725","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Unzipping the dataset file combined_faces.zip\n","\n","combined_faces_zip_path = \"/content/drive/My Drive/Faces_Dataset/ZIPPED_DATASET/combined_faces_train_augmented.zip\"\n","\n","with ZipFile(combined_faces_zip_path, 'r') as myzip:\n","    myzip.extractall()\n","    print('Done unzipping combined_faces_train_augmented.zip')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Done unzipping combined_faces_train_augmented.zip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yu50xj5jU8I6"},"source":["# Importing the augmented training dataset and testing dataset to create tensors of images using the filename paths.\n","\n","train_aug_df = pd.read_csv(\"/content/drive/My Drive/age_classification/input_output/images_filenames_labels_train_augmented.csv\")\n","test_df = pd.read_csv(\"/content/drive/My Drive/age_classification/input_output/images_filenames_labels_test.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgRflp79U8N1","executionInfo":{"status":"ok","timestamp":1602513117312,"user_tz":-330,"elapsed":269190,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"19ba71fe-4f58-4c15-aa7b-312073ec5702","colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["# Dropping the age column since classes of age-ranges have been re-distributed from 11 to 7 classes.\n","\n","train_aug_df.drop(columns=['target'], inplace=True)\n","train_aug_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>age</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/content/combined_faces_train_augmente...</td>\n","      <td>80</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/content/combined_faces_train_augmente...</td>\n","      <td>74</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/content/combined_faces_train_augmente...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/content/combined_faces_train_augmente...</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/content/combined_faces_train_augmente...</td>\n","      <td>29</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            filename  age\n","0  /content/content/combined_faces_train_augmente...   80\n","1  /content/content/combined_faces_train_augmente...   74\n","2  /content/content/combined_faces_train_augmente...    4\n","3  /content/content/combined_faces_train_augmente...   29\n","4  /content/content/combined_faces_train_augmente...   29"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"BcFPQNl7U8SZ","executionInfo":{"status":"ok","timestamp":1602513122665,"user_tz":-330,"elapsed":5323,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"774dd459-f407-4a79-86b6-7675ed8489bb","colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["# Dropping the age column since classes of age-ranges have been re-distributed from 11 to 7 classes.\n","\n","test_df.drop(columns=['target'], inplace=True)\n","test_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>age</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/content/combined_faces/8_390.jpg</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/content/combined_faces/42_80.jpg</td>\n","      <td>42</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/content/combined_faces/38_267.jpg</td>\n","      <td>38</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/content/combined_faces/35_37.jpg</td>\n","      <td>35</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/content/combined_faces/7_115.jpg</td>\n","      <td>7</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                     filename  age\n","0   /content/content/combined_faces/8_390.jpg    8\n","1   /content/content/combined_faces/42_80.jpg   42\n","2  /content/content/combined_faces/38_267.jpg   38\n","3   /content/content/combined_faces/35_37.jpg   35\n","4   /content/content/combined_faces/7_115.jpg    7"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"5D3RBtwwU8W1","executionInfo":{"status":"ok","timestamp":1602513122666,"user_tz":-330,"elapsed":5307,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"505ca1d6-8015-4ef5-82ea-f6d7f322cc70","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_aug_df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(234400, 2)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"J4Qz1-9mU8bN","executionInfo":{"status":"ok","timestamp":1602513122667,"user_tz":-330,"elapsed":5300,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"9272977b-4492-4a48-c387-9711ac7d024a","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["test_df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10046, 2)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"QqlcD7x6Yjcd"},"source":["# Defining a function to return the class labels corresponding to the re-distributed 7 age-ranges.\n","\n","def class_labels_reassign(age):\n","\n","    if 1 <= age <= 2:\n","        return 0\n","    elif 3 <= age <= 9:\n","        return 1\n","    elif 10 <= age <= 20:\n","        return 2\n","    elif 21 <= age <= 27:\n","        return 3\n","    elif 28 <= age <= 45:\n","        return 4\n","    elif 46 <= age <= 65:\n","        return 5\n","    else:\n","        return 6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LROsGcIaYjVr"},"source":["train_aug_df['target'] = train_aug_df['age'].map(class_labels_reassign)\n","test_df['target'] = test_df['age'].map(class_labels_reassign)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j51hK9VIaxo1","executionInfo":{"status":"ok","timestamp":1602513122669,"user_tz":-330,"elapsed":5289,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"a8a2bd02-40e2-447a-a03c-d95a2c7a9b7a","colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["train_aug_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>age</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/content/combined_faces_train_augmente...</td>\n","      <td>80</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/content/combined_faces_train_augmente...</td>\n","      <td>74</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/content/combined_faces_train_augmente...</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/content/combined_faces_train_augmente...</td>\n","      <td>29</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/content/combined_faces_train_augmente...</td>\n","      <td>29</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            filename  age  target\n","0  /content/content/combined_faces_train_augmente...   80       6\n","1  /content/content/combined_faces_train_augmente...   74       6\n","2  /content/content/combined_faces_train_augmente...    4       1\n","3  /content/content/combined_faces_train_augmente...   29       4\n","4  /content/content/combined_faces_train_augmente...   29       4"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"csZb53SYaxgu","executionInfo":{"status":"ok","timestamp":1602513122669,"user_tz":-330,"elapsed":5281,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"593f64a4-f2c0-4b18-f252-47436f17f509","colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["test_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>age</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/content/combined_faces/8_390.jpg</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/content/combined_faces/42_80.jpg</td>\n","      <td>42</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/content/combined_faces/38_267.jpg</td>\n","      <td>38</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/content/combined_faces/35_37.jpg</td>\n","      <td>35</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/content/combined_faces/7_115.jpg</td>\n","      <td>7</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                     filename  age  target\n","0   /content/content/combined_faces/8_390.jpg    8       1\n","1   /content/content/combined_faces/42_80.jpg   42       4\n","2  /content/content/combined_faces/38_267.jpg   38       4\n","3   /content/content/combined_faces/35_37.jpg   35       4\n","4   /content/content/combined_faces/7_115.jpg    7       1"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"jWpwyHjzYjQN","executionInfo":{"status":"ok","timestamp":1602513122670,"user_tz":-330,"elapsed":5273,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"73b5ec17-a5f3-40b6-c3fc-b45326a12324","colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["train_aug_df['target'].value_counts(normalize=True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4    0.279394\n","3    0.199829\n","5    0.167193\n","0    0.095307\n","2    0.093643\n","1    0.084087\n","6    0.080546\n","Name: target, dtype: float64"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"KUS5oAuRavFJ","executionInfo":{"status":"ok","timestamp":1602513122671,"user_tz":-330,"elapsed":5266,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"472b1978-13f7-4d52-e66f-8a8b9d4f601e","colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["test_df['target'].value_counts(normalize=True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4    0.279415\n","3    0.199781\n","5    0.167131\n","0    0.095361\n","2    0.093669\n","1    0.084113\n","6    0.080530\n","Name: target, dtype: float64"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"sJSY34ShavL_"},"source":["# Converting the filenames and target class labels into lists for augmented train and test datasets.\n","\n","train_aug_filenames_list = list(train_aug_df['filename'])\n","train_aug_labels_list = list(train_aug_df['target'])\n","\n","test_filenames_list = list(test_df['filename'])\n","test_labels_list = list(test_df['target'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S7ye8s5savP5"},"source":["# Creating tensorflow constants of filenames and labels for augmented train and test datasets from the lists defined above.\n","\n","train_aug_filenames_tensor = tf.constant(train_aug_filenames_list)\n","train_aug_labels_tensor = tf.constant(train_aug_labels_list)\n","\n","test_filenames_tensor = tf.constant(test_filenames_list)\n","test_labels_tensor = tf.constant(test_labels_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ALPko3Xavbo"},"source":["# Defining a function to read the image, decode the image from given tensor and one-hot encode the image label class.\n","# Changing the channels para in tf.io.decode_jpeg from 3 to 1 changes the output images from RGB coloured to grayscale.\n","\n","num_classes = 7\n","\n","def _parse_function(filename, label):\n","    \n","    image_string = tf.io.read_file(filename)\n","    image_decoded = tf.io.decode_jpeg(image_string, channels=1)    # channels=1 to convert to grayscale, channels=3 to convert to RGB.\n","    # image_resized = tf.image.resize(image_decoded, [200, 200])\n","    label = tf.one_hot(label, num_classes)\n","\n","    return image_decoded, label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OTyXZ2tUavZh"},"source":["# Getting the dataset ready for the neural network.\n","# Using the tensor vectors defined above, accessing the images in the dataset and passing them through the function defined above.\n","\n","train_aug_dataset = tf.data.Dataset.from_tensor_slices((train_aug_filenames_tensor, train_aug_labels_tensor))\n","train_aug_dataset = train_aug_dataset.map(_parse_function)\n","train_aug_dataset = train_aug_dataset.batch(512)    \n","test_dataset = tf.data.Dataset.from_tensor_slices((test_filenames_tensor, test_labels_tensor))\n","test_dataset = test_dataset.map(_parse_function)\n","test_dataset = test_dataset.batch(512)    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kcW44JspavWb","executionInfo":{"status":"ok","timestamp":1602513332687,"user_tz":-330,"elapsed":2924,"user":{"displayName":"Kogatam Thanmai","photoUrl":"","userId":"16086348149265634020"}},"outputId":"9c1c1e0d-17dc-4172-fa9b-7022ceb719ee","colab":{"base_uri":"https://localhost:8080/","height":521}},"source":["# Defining the architecture of the sequential neural network.\n","\n","final_cnn = Sequential()\n","\n","# Input layer with 32 filters, followed by an AveragePooling2D layer.\n","final_cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=(200, 200, 1)))    \n","final_cnn.add(AveragePooling2D(pool_size=(2,2)))\n","# Three Conv2D layers with filters increasing by a factor of 2 for every successive Conv2D layer.\n","final_cnn.add(Conv2D(filters=64, kernel_size=3, activation='relu'))\n","final_cnn.add(AveragePooling2D(pool_size=(2,2)))\n","final_cnn.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\n","final_cnn.add(AveragePooling2D(pool_size=(2,2)))\n","final_cnn.add(Conv2D(filters=256, kernel_size=3, activation='relu'))\n","final_cnn.add(AveragePooling2D(pool_size=(2,2)))\n","# GlobalAveragePooling2D layer gives no. of outputs equal to no. of filters in last Conv2D layer above (256).\n","final_cnn.add(GlobalAveragePooling2D())\n","final_cnn.add(Dense(132, activation='relu'))\n","final_cnn.add(Dense(7, activation='softmax'))\n","final_cnn.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_4 (Conv2D)            (None, 198, 198, 32)      320       \n","_________________________________________________________________\n","average_pooling2d_4 (Average (None, 99, 99, 32)        0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 97, 97, 64)        18496     \n","_________________________________________________________________\n","average_pooling2d_5 (Average (None, 48, 48, 64)        0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 46, 46, 128)       73856     \n","_________________________________________________________________\n","average_pooling2d_6 (Average (None, 23, 23, 128)       0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 21, 21, 256)       295168    \n","_________________________________________________________________\n","average_pooling2d_7 (Average (None, 10, 10, 256)       0         \n","_________________________________________________________________\n","global_average_pooling2d_1 ( (None, 256)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 132)               33924     \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 7)                 931       \n","=================================================================\n","Total params: 422,695\n","Trainable params: 422,695\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uBG5MY8zavTn"},"source":["# Compiling the above created CNN architecture.\n","\n","final_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nWy7fRfHfu8L"},"source":["# Creating a TensorBoard callback object and saving it at the desired location.\n","\n","tensorboard = TensorBoard(log_dir=f\"/content/drive/My Drive/age_classification/input_output/cnn_logs/final_cnn\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3pKfqLLbS3ld"},"source":["I will also use ***ModelCheckpoint*** as a callback while training the final CNN model so as to be able to save the model as it continues training and improving in performance over 60 epochs."]},{"cell_type":"code","metadata":{"id":"X6dQDDhmfuzo"},"source":["# Creating a ModelCheckpoint callback object to save the model according to the value of val_accuracy.\n","\n","checkpoint = ModelCheckpoint(filepath=f\"/content/drive/My Drive/age_classification/input_output/final_cnn_model_checkpoint.h5\",\n","                             monitor='val_accuracy',\n","                             save_best_only=True,\n","                             save_weights_only=False,\n","                             verbose=1\n","                            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hU4LJT6xYjJp","outputId":"e37fcee1-06c4-4301-b7e5-41ab82e44fb8","colab":{"base_uri":"https://localhost:8080/","height":675}},"source":["# Fitting the above created CNN model.\n","\n","final_cnn_history = final_cnn.fit(train_aug_dataset,\n","                                  batch_size=512,\n","                                  validation_data=test_dataset,\n","                                  epochs=60,\n","                                  callbacks=[tensorboard, checkpoint],\n","                                  shuffle=False    # shuffle=False to reduce randomness and increase reproducibility\n","                                 )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/60\n","  1/458 [..............................] - ETA: 0s - loss: 3.8567 - accuracy: 0.0957WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n","Instructions for updating:\n","use `tf.profiler.experimental.stop` instead.\n","  2/458 [..............................] - ETA: 4:07 - loss: 7.5543 - accuracy: 0.0850WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1062s vs `on_train_batch_end` time: 0.9762s). Check your callbacks.\n","458/458 [==============================] - ETA: 0s - loss: 1.7964 - accuracy: 0.3227\n","Epoch 00001: val_accuracy improved from -inf to 0.41061, saving model to /content/drive/My Drive/age_classification/input_output/final_cnn_model_checkpoint.h5\n","458/458 [==============================] - 242s 528ms/step - loss: 1.7964 - accuracy: 0.3227 - val_loss: 1.4808 - val_accuracy: 0.4106\n","Epoch 2/60\n","458/458 [==============================] - ETA: 0s - loss: 1.4315 - accuracy: 0.4180\n","Epoch 00002: val_accuracy improved from 0.41061 to 0.47651, saving model to /content/drive/My Drive/age_classification/input_output/final_cnn_model_checkpoint.h5\n","458/458 [==============================] - 177s 386ms/step - loss: 1.4315 - accuracy: 0.4180 - val_loss: 1.2763 - val_accuracy: 0.4765\n","Epoch 3/60\n","458/458 [==============================] - ETA: 0s - loss: 1.2564 - accuracy: 0.4854\n","Epoch 00003: val_accuracy improved from 0.47651 to 0.51483, saving model to /content/drive/My Drive/age_classification/input_output/final_cnn_model_checkpoint.h5\n","458/458 [==============================] - 160s 350ms/step - loss: 1.2564 - accuracy: 0.4854 - val_loss: 1.1932 - val_accuracy: 0.5148\n","Epoch 4/60\n","458/458 [==============================] - ETA: 0s - loss: 1.1394 - accuracy: 0.5284\n","Epoch 00004: val_accuracy improved from 0.51483 to 0.55087, saving model to /content/drive/My Drive/age_classification/input_output/final_cnn_model_checkpoint.h5\n","458/458 [==============================] - 166s 363ms/step - loss: 1.1394 - accuracy: 0.5284 - val_loss: 1.0971 - val_accuracy: 0.5509\n","Epoch 5/60\n","458/458 [==============================] - ETA: 0s - loss: 1.0588 - accuracy: 0.5596\n","Epoch 00005: val_accuracy improved from 0.55087 to 0.57376, saving model to /content/drive/My Drive/age_classification/input_output/final_cnn_model_checkpoint.h5\n","458/458 [==============================] - 162s 355ms/step - loss: 1.0588 - accuracy: 0.5596 - val_loss: 1.0318 - val_accuracy: 0.5738\n","Epoch 6/60\n","458/458 [==============================] - ETA: 0s - loss: 1.0058 - accuracy: 0.5797\n","Epoch 00006: val_accuracy improved from 0.57376 to 0.57993, saving model to /content/drive/My Drive/age_classification/input_output/final_cnn_model_checkpoint.h5\n","458/458 [==============================] - 162s 353ms/step - loss: 1.0058 - accuracy: 0.5797 - val_loss: 1.0007 - val_accuracy: 0.5799\n","Epoch 7/60\n","458/458 [==============================] - ETA: 0s - loss: 0.9530 - accuracy: 0.5993\n","Epoch 00007: val_accuracy improved from 0.57993 to 0.60054, saving model to /content/drive/My Drive/age_classification/input_output/final_cnn_model_checkpoint.h5\n","458/458 [==============================] - 163s 355ms/step - loss: 0.9530 - accuracy: 0.5993 - val_loss: 0.9488 - val_accuracy: 0.6005\n","Epoch 8/60\n","458/458 [==============================] - ETA: 0s - loss: 0.9197 - accuracy: 0.6133\n","Epoch 00008: val_accuracy improved from 0.60054 to 0.63050, saving model to /content/drive/My Drive/age_classification/input_output/final_cnn_model_checkpoint.h5\n","458/458 [==============================] - 162s 355ms/step - loss: 0.9197 - accuracy: 0.6133 - val_loss: 0.8795 - val_accuracy: 0.6305\n","Epoch 9/60\n","440/458 [===========================>..] - ETA: 5s - loss: 0.8802 - accuracy: 0.6291"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RP7vxY_6YjCM"},"source":["# Checking the train and test loss and accuracy values from the neural network above.\n","\n","train_loss = final_cnn_history.history['loss']\n","test_loss = final_cnn_history.history['val_loss']\n","train_accuracy = final_cnn_history.history['accuracy']\n","test_accuracy = final_cnn_history.history['val_accuracy']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"COkki9JUU8kf"},"source":["# Plotting a line chart to visualize the loss and accuracy values by epochs.\n","\n","fig, ax = plt.subplots(ncols=2, figsize=(15,7))\n","\n","ax = ax.ravel()\n","\n","ax[0].plot(train_loss, label='Train Loss', color='royalblue', marker='o', markersize=5)\n","ax[0].plot(test_loss, label='Test Loss', color = 'orangered', marker='o', markersize=5)\n","\n","ax[0].set_xlabel('Epochs', fontsize=14)\n","ax[0].set_ylabel('Categorical Crossentropy', fontsize=14)\n","\n","ax[0].legend(fontsize=14)\n","ax[0].tick_params(axis='both', labelsize=12)\n","\n","ax[1].plot(train_accuracy, label='Train Accuracy', color='royalblue', marker='o', markersize=5)\n","ax[1].plot(test_accuracy, label='Test Accuracy', color='orangered', marker='o', markersize=5)\n","\n","ax[1].set_xlabel('Epochs', fontsize=14)\n","ax[1].set_ylabel('Accuracy', fontsize=14)\n","\n","ax[1].legend(fontsize=14)\n","ax[1].tick_params(axis='both', labelsize=12)\n","\n","fig.suptitle(x=0.5, y=0.92, t=\"Lineplots showing loss and accuracy of CNN model by epochs\", fontsize=16)\n","\n","# Exporting plot image in PNG format.\n","plt.savefig('/content/drive/My Drive/age_classification/plot_images/final_cnn_loss_accuracy.png', bbox_inches='tight');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F75NJtVJU8oD"},"source":["# Evaluating the model on test dataset.\n","\n","final_cnn_score = final_cnn.evaluate(test_dataset, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5JdZYFMU8s0"},"source":["# Printing the relevant score summary.\n","\n","final_cnn_labels = final_cnn.metrics_names\n","print(f'CNN model {final_cnn_labels[0]} \\t\\t= {round(final_cnn_score[0], 3)}')\n","print(f'CNN model {final_cnn_labels[1]} \\t= {round(final_cnn_score[1], 3)}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fatkGFAkrjCF"},"source":["# Saving the model as a h5 file for possible use later.\n","\n","final_cnn.save(f\"/content/drive/My Drive/age_classification/input_output/final_cnn_model_acc_{round(final_cnn_score[1], 3)}.h5\", save_format='h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R1e1uYqHU8wx"},"source":["# Generating predictions from the model above.\n","\n","final_cnn_pred = final_cnn.predict(test_dataset)\n","final_cnn_pred = final_cnn_pred.argmax(axis=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EpdV9V1EU81O"},"source":["len(final_cnn_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aYLmUsxcU85i"},"source":["len(test_labels_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T6hAs7SlU9Zi"},"source":["# Generating a confusion matrix based on above predictions.\n","\n","conf_mat = confusion_matrix(test_labels_list, final_cnn_pred)\n","conf_mat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uACH8n6fU95F"},"source":["# Defining a function to plot the confusion matrix in a grid for easier visualization.\n","\n","def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', export_as='confusion_matrix', cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    # print(cm)\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title, fontsize=16)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True labels', fontsize=14)\n","    plt.xlabel('Predicted labels', fontsize=14)\n","\n","    # Exporting plot image in PNG format.\n","    plt.savefig(f'/content/drive/My Drive/age_classification/plot_images/{export_as}.png', bbox_inches='tight');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1XROLD_U91Z"},"source":["# Plotting the confusion matrix using the function defined above.\n","\n","cm_plot_labels = ['1-2', '3-9', '10-20', '21-27', '28-45', '46-65', '66-116']\n","\n","plt.figure(figsize=(16,8))\n","plot_confusion_matrix(conf_mat, cm_plot_labels, normalize=True,\n","                      title=\"Confusion Matrix based on predictions from CNN model\",\n","                      export_as=\"final_cnn_conf_mat_norm\"\n","                     )\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b8SX7GwcU9vz"},"source":["# Plotting the confusion matrix using the function defined above.\n","\n","cm_plot_labels = ['1-2', '3-9', '10-20', '21-27', '28-45', '46-65', '66-116']\n","\n","plt.figure(figsize=(16,8))\n","plot_confusion_matrix(conf_mat, cm_plot_labels, normalize=False,\n","                      title=\"Confusion Matrix based on predictions from CNN model\",\n","                      export_as=\"final_cnn_conf_mat\"\n","                     )\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVCOciqHTgYF"},"source":["As seen in the lineplots above, the **model peaked in performance at epoch 54 of 60**, with **training accuracy = 90.44% and testing accuracy = 82.97%** The accuracy scores improved significantly, although with a **slight degree of over-fit, which may still be acceptable**. The normalized confusion matrix above also shows a **significant reduction in the misclassification for the middle age-ranges of 26–65**.\n"]},{"cell_type":"markdown","metadata":{"id":"6eHgY5EOez2i"},"source":["## Deep Learning: Summary and Limitations"]},{"cell_type":"markdown","metadata":{"id":"3cpS3994e9GM"},"source":["Below is the **final summary of all the CNN models built so far**.\n","\n","![summary_table_final.PNG](https://drive.google.com/uc?export=view&id=1VzZygldRJen6HgRR27m9J_VKI2bKD1IA)\n","\n","As with any data science workflow, the deep learning approach presented above does have its own limitations as well. For instance, the original datasets used in this project only had about 33,000 images when combined together. Even though the training dataset was augmented to increase it's size from 23,440 images to 234,400 images, there is always a possibility that an **even larger training dataset with more variation in the images would have resulted in even better results**. Another approach to this project could be to use **Transfer Learning (using the architecture and layer weights from a pre-trained neural network)** instead of creating and training a neural network from scratch.\n","\n","Compared to the **traditional Machine Learning** methodology and approach however, the CNN model performance is significantly better (**results summary shown below**).\n","\n","![summary_table_traditional_ml.png](https://drive.google.com/uc?export=view&id=1kc7idDm-1QmSTfdmk592NpMXWPU8moGu)"]}]}